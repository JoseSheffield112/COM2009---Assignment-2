%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COM2009-3009 Human-Machine Interaction and Robotics
% Lab Assignment #2
% Prof. Roger K. Moore & Dr. Mike Mangan
% University of Sheffield
% 5 March 2019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[hidelinks,a4paper,11pt]{article}

\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{mdframed}
\usepackage{enumitem,amssymb}
\usepackage{float}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}
\hspace{-2.5pt}}
\newcounter{question}
\newcommand\myq{\refstepcounter{question}\thequestion}
\usepackage{gensymb}


\begin{document}


\begin{titlepage}

\begin{center}
{\LARGE University of Sheffield}\\[1cm]
\huge {\bfseries COM2009-3009\\Human-Machine Interaction and Robotics}\\[1cm]
\includegraphics[width=5cm]{tuoslogo.png}\\[1cm]
{\LARGE Lab Assignment \#2\\\textbf{`Multi-Layer Control'}}\\[2cm]

% DELETE THESE LINES FOR THE STUDENT VERSION
%vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
%{\huge \bfseries Instructions}\\[0.5cm]
%{\Large Prof. Roger K. Moore \& Dr. Mike Mangan}\\[1cm]
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

% UNCOMMENT THESE LINES FOR THE STUDENT VERSION
%vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
%% EDIT YOUR NAMES
%{\Large John Smith}\\
%{\Large David Jones}\\
%{\Large Scott Macdonald}\\[1cm]

%% EDIT THE ID OF YOUR ROBOT
%{\LARGE Robot: XY}\\[1cm]
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

{\LARGE Department of Computer Science}\\
{\Large \today}
\end{center}

\end{titlepage}

\tableofcontents
\newpage

\section{Overview (please read carefully)}

In Lab Assignment \#1 you investigated a single-layer negative-feedback control system.  In particular, you developed a `Proportional-Integral-Derivative' (PID) controller that was capable of steering a robot through a maze consisting of a long winding corridor.  For Lab Assignment \#2 your challenge is to navigate a more complex environment containing real-world hazards - and, in order to do this, you will need to add new layers of functionality to your existing robot through a  \emph{multi-layer} control system.

This programming assignment is worth 30\% of the overall course mark.

\subsection{Logistics}

As before, the lab classes for COM2009-3009 take place in Computer Room 3 in the Diamond (DIA-CR3) from 13:00 to 14:50 on Mondays (for Group A) and 09:00 to 10:50 on Wednesdays (for Group B).  \textbf{The teams have been re-randomised from Assignment 1.  Your new team and robot are listed on MOLE}.

The assignment will again be judged in a league-based competition, and assessed on the basis of a group report (prepared using \LaTeX).

The deadline for handing-in the group report (via MOLE) is \ldots

\begin{center}\textbf{{\large {\color{red}Midnight Friday 17\textsuperscript{th} May 2019 (week 12)}}}\end{center}

{\bfseries Reminder:}  \emph{You may \underline{not} take a robot out of DIA-CR3 under any circumstances!}

{\bfseries Reminder:}  \emph{Please be careful in and around the robot arena; the robots can move unexpectedly and the arena itself is high enough to be a trip hazard.  There is a risk assessment for this in the folder on the teaching podium.}

{\bfseries Reminder:}  \emph{At the end of each lab session, turn off your robot and return it to the correct charging cabinet, with the microcomputer (brick) being placed on charge by plugging in the power adapter in the centre section of the cabinet.}

{\bfseries Reminder:}  \emph{If you have customised your robot, take a photo (as a memory aid) and return it to its original `Driving Base' configuration and place all the Lego back in its box\footnote{This is necessary because, as before, the robots are shared between Group A and Group B.}}.


\subsection{Provided Materials}

In addition to these instructions, you have been provided with a \texttt{.zip} file containing a number of items\footnote{The provided \texttt{SystemDiagram.jpg} is simply a placeholder that you will replace with your own diagram.} that you will need for this assignment.  In particular, you have been supplied with a \LaTeX\ template - \texttt{COM2009-3009\_Assignment-2\_Robot-XY.tex} - which will you use to compile your response sheet.

\begin{todolist}
	\item Before you start working with \LaTeX, edit the filename of the provided \texttt{.tex} file by replacing \texttt{XY} with your robot id.
\end{todolist}

As before, your edited \texttt{.tex} file should compile to produce a \texttt{.pdf} document with your names on the front cover, followed by your responses to a number of questions.  You will be submitting the \texttt{.pdf} file when you have completed the assignment, \emph{not} the \texttt{.tex} file.

\subsection{Hand-In Procedure}

Once you have completed the assignment, one member of each group should submit your response sheet (in \texttt{.pdf} format) via MOLE.  You do NOT need to submit a paper copy, and do NOT submit your \LaTeX\ source files.

The filename should be of the form \texttt{COM2009-3009\_Assignment-2\_Robot-XY.pdf}.  Please make sure that your names and the id of your robot are shown correctly on the front page of your report.

Standard departmental penalties apply for late hand-in\footnote{\url{http://www.dcs.shef.ac.uk/intranet/teaching/public/assessment/latehandin.html}} and plagiarism\footnote{\url{http://www.dcs.shef.ac.uk/intranet/teaching/public/assessment/plagiarism.html}}.

Feedback (including provisional marks) will be provided via MOLE within three working weeks of the hand-in date.

The deadline for handing-in this assignment (via MOLE) is \ldots

\begin{center}\textbf{{\large {\color{red}Midnight Friday 17\textsuperscript{th} May 2019 (week 12)}}}\end{center}

\subsection{Increase in expected levels of autonomy}
In Lab Assignment \#1, care was taken to guide you through the various aspects of design that were required to build a maze-running robot.  This included an introduction to the Lego EV3 hardware, sample code for programming your robot, and detailed guides on how to calibrate and program a PID controller.  

In Lab Assignment \#2, we expect teams to work in a more independent manner to find their own solutions to the problems posed.  Part I below provides guides as to the type of behaviours you will need to implement and also links to lectures where common solutions to similar problems were covered.  We strongly encourage students to research novel solutions, or even come up with their own.  Novel solutions can be discussed in the submitted report.

Also, we cannot guarantee extra lab sessions as for Assignment 1 so please plan accordingly.  It is very important to manage your development time on each task to ensure that you get to the end in time for the competition.

{\bfseries Reminder:}  \emph{Lab Demonstrators are provided to guide your development and address technical questions that you might experience.  Given the expectation of independent development they will not be able to debug code or provide full solutions as they will be specific to your robot.}



\newpage
\section{Part I: Building A Multilayer Robot Control Architecture}

\subsection{The Search and Assist Challenge}

In Assignment 2 you will again develop a robot to compete against your peers in a simulated life-like challenge within the same test arena as Assignment 1.  Your task is to program a 'search and assist robot' that must find a stricken individual in an unstructured environment (test arena with randomly distributed obstacles) and remain there until further assistance arrives.  The individual's location can be identified by an alarm beacon (light source). Your robot must navigate to the individual's location while avoiding contact with obstacles and the arena walls.  Your robot's start position, and the positions of the target and obstacles will not be revealed until the competition session so you will have to design a controller that works in many conditions.  You shall be ranked by the number of seconds that your robot spends at the target location within the 2 minutes allowed for each robot.

The following sections guide you through development of the various control behaviours that your robot will require to resolve this challenge but we expect and encourage teams to develop novel solutions that improve performance further.  You will also notice that there is increased requirement to design and document unit tests to demonstrate the functioning of your subsystems.  The design of the unit tests is for you to define - as you would in a real engineering or R\&D position. 

\subsection{Building An Obstacle Avoidance Behaviour}

In Assignment 1 you developed a PID controller to follow a maze without touching walls i.e. avoiding obstacles.  In your new group, use the lessons learned from the last assignment to give your robot an obstacle avoidance capability.  

\begin{todolist}
	\item Program a PID-based obstacle avoidance behaviour.  Take into account the hardware designs that produced the best results in the maze following competition.  Refer to Assignment 1 documentation for reminders on calibrating your controller.
	\item Design and document unit tests to demonstrate that your robot succeeds in this task.  
\end{todolist}

{\bfseries Question \myq\ Describe your PID-based obstacle avoidance architecture with accompanying unit tests that demonstrate that it functions correctly. \emph{(worth up to 5 marks)}}
\\
\begin{mdframed}
Design:\par
Hardware:\par
We initially used the predominant lab 1 robot configuration (front facing perpendicular ultrasonic sensors), as we were all familiar with this configuration and had the same working PID code. However, during the unit testing (see below), we had a few instances where the robot hit an obstacle head on due to it being in it's blind spot. We decided to make a new design so we could aim for the extra 5 marks of avoiding obstacles.
The new design uses a rotating ultrasonic sensor, that continuously alternates between scanning the front, right and left of the robot. Whilst this method still has an inherent weakness of reacting slower to obstacles on a side the robot isn't facing, we have taken steps to decrease the occurrence of this by limiting the speed and increasing the minimum distance in the PID software implementation (tests below).

Software design:\par
For the first hardware design we used our previous labs PID avoidance code, which subtracted the left measurement from the right and used this value to calculate the wheel speed (this method introduced some errors in large spaces which we have overcome with our second design).
For our second implementation of a rotating sensor, we adapted the PID code from our first lab assignment so that the robot wheels move at a base speed; if a value from one of the robots sides is within the minimum distance(set to 5cm during testing), it will calculate a speed value with that distance and deduct it from the opposite wheels base speed (similar to how the Braitenberg "explorer robot" operates). When the front value is within the minimum distance, the robot stops and backs ups, then measures its left and right side; it then moves to the side with the highest value, favoring the left if both values are at the sensor maximum of 2550.

Unit tests:
We performed the same tests for each method for fair comparison, so by the end we could make a clear decision on the most preferable avoidance system.

\begin{tabular}{ |p{2cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : Ultrasonic sensors perpendicular} \\
 \hline
 Scenario & Reason & Expected outcome & Actual outcome & Notes\\
 \hline
 Placing the robot in the middle of the arena & To test if PID avoidance code is working & Robot moves and avoids any oncoming obstacles & Robot moved and avoided obstacles & We ran this twice, as in first test it collided with an obstacle in its blindspot.\\
 \hline
 Placing the robot facing a corner & To test whether the robot can get itself out without a collision & The robot gets itself out of the corner with no collisions & The robot collided with the wall a few times and needed a rescue & \\
 \hline
 Placing the robot in front of an obstacle& So we know if we have to worry about the robots blindspot & Avoids collision with the obstacle & The robot collided with the obstacle & After a few tests where we varied the distance, we weren't happy with the results. \\
 \hline
\end{tabular}

Due to the faults identified above with the previous implementation of the robot, we pursued a different design, the unit tests of which are below.

\begin{tabular}{ |p{2cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : Rotating ultrasonic sensor} \\
 \hline
 Scenario & Reason & Expected outcome & Actual outcome & Notes\\
 \hline
 Placing robot in the middle of the arena & To test whether PID avoidance code is working & Robot moves about arena avoiding obstacles & Robot moved and avoided obstacles & There were a few instances were other robots crashed into it on its blind spot which we have decided to leave since it reacted well to stationary obstacles \\
 \hline
 Placing the robot in a corner & To see if it can leave it without collisions or rescues & Robot should back up and turn another direction & Robot did as intended & \\
 \hline
 Placing the robot in front of an obstacle & To test one of the main flaws in previous design & Robot backs up and turns another direction & Robot did as intended & \\
 \hline
\end{tabular}

 It is clear from the 2 unit tests that our second design is working best. Whilst this designs has some weakness, such as the constant shaking of the ultrasonic sensor and the slow reaction times, we believe all designs have inherent flaws that cannot be overcome with the time and resources we have been allocated, and the current design overcomes various issues we'd have with other implementations, such as significantly reducing the robots blind spots and being able to make an effective decision on were to turn if the robot is in a very constricted spot.
 
 
 Additional insights:\par
 If time allowed, we would have explored a way of increasing the accuracy of the measurements (potentially with 2 rotating sensors opposite each other). However, the extra time needed to implement this with the PID software would be too long.
 Furthermore, another solution we would have liked to have tested (idea collected from dyson hoover guest lecture) would be to use a touch sensor on both sides of the robot, so when it's close to a collision on a side, the sensor would identify this and the robot would steer away from it. This solution would easily overcome burden placed on the rotating ultrasonic sensor design as well as reducing the probability of any collisions.
 
 However, to further the implemented design without any hardware changes we'd have liked to implement 2 new features that we did not have time to develop due to their large scope and complexity. Firstly, we would have liked to increase the number of distance measurements the sensor captures - currently, we only rotate between 3 sides roughly every 4 seconds due to the weakness of the motor. A second feature we'd like to add is making an internal map, where the robot keeps a record of the distance from each measured obstacle in its front path.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding A Search Behaviour}

Your robot will now be able to react to obstacles but needs a guidance strategy to explore the arena in search of the individual (light).  Thus, your task is to add a search behaviour allowing your robot to explore the environment.

\begin{todolist}
	\item Program a search behaviour that will drive your robot around the arena.  Consider the various search algorithms that you might use to most efficiently explore the test arena.  \textit{Hint: search strategies were covered in Lecture 6 but there are many others available.}
	\item In your robot kits you will find a Gyro Sensor which can be used to monitor turning angles and angular accelerations.  This may or may not be useful to your specific search strategy.  Refer to the online EV3 documentation (\url{https://ev3dev-lang.readthedocs.io/projects/python-ev3dev/en/stable/spec.html}), to understand how to access the data from the Gyro Sensor.
	\item Again, design and document unit tests to demonstrate that your search strategy performs as expected.  If time allows, you may want to implement and compare two different search algorithms to assess which is best suited to the task, or even implement a novel hybrid search strategy. 
\end{todolist}

{\bfseries Question \myq\ Describe the design of your search algorithm including discussion of design choices and what testing you have implemented. \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Hardware design:\par
For our spiral search we secured a light sensor at the front of the robot to minimize the swaying of the sensor when doing sharp turns so our values were more accurate. We placed the sensor at the front of the robot so there was no measurement bias (as placing it on a side would blind it to values from the opposite side).
When using the wanderer search, we paired our light sensor with the ultrasonic sensor on the rotation motor, so it's easier for the robot to take recordings of light values around it.

Algorithm design:\par
We initially coded a simple spiral search with the spiralling radius gradually increasing over time, where the beaconing behavior would take over once the robot is close enough to light source.
We felt confident the robot could eventually find the light source with this search pattern, however due to the time restriction and the varying map layout (which could prevent us from effectively using a spiral search), we decided to code another pattern so we could compare the two and be confident the one chosen was best.
We decided to compare the spiral search to a wanderer search. This search pattern uses the PID avoidance system aggregated with a color sensor, to "wander" around the arena whilst favouring areas with higher light intensity. This search pattern is best suited to our robot due to the rotating ultrasonic sensor.

Below are the unit tests of both algorithms we designed. We did 4 trials for each pattern, keeping the distance the same for each pattern, but varying the distance between tests. We measured the time it took for the robot to reach a light value above 35, where the beaconing behavior would then be expected to take over.\par

\begin{tabular}{ |p{3cm}|p{3cm}|p{2.5cm}|  }
 \hline
 \multicolumn{3}{|c|}{Unit tests : spiral search vs wander search} \\
 \hline
 Method & Large & Notes\\
 \hline
 Spiral search & Mean = 2:05 & \\
 \hline
 Wanderer search & Mean = 57  & \\
 \hline
\end{tabular}

It is worthwhile to consider that all these tests were conducted during lab times, when everyone else was also using their robots, hence, there is a certain amount of interference in the times. However, it is abundantly clear from the tests above that the best search pattern we can use is the wanderer search.

During later stages of development we aggregated a function which every 30 execution cycles, will do a 360 degree spin on the spot, recording a light value about every 5 degrees, and then heads in the direction of the highest light value. We did this for two reasons; firstly, so that if the robot gets stuck in a loop due to the obstacle avoidance, this should help it break out of it. Secondly, so that if the robots wanderer search path keeps facing the back of the light, we can introduce that measurement without too much interference.

Additional insights:\par
Like the PID avoidance issue, the robot would work much more efficiently if it was able to collect more light samples, which would require the rotation motor to move faster.
If we had access to another light sensor, we would have liked to use opposite each other, as this way we can effectively reduce the likely hood of the robot turning its back on the light ignoring it. Plus, we could use this so the robot knows when it is following the light from the window (the light from the window would caue both light value measurements to be quite similar, whilst the light source is not strong enough to produce this effect).
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Switching Behaviours Using A Subsumption Architecture}

Your robot should now have two functioning, single layer behaviours:  obstacle avoidance and search.  You now need to consider how your robot will select which strategy to apply and when.  

\begin{todolist}
	\item With reference to lecture 4 implement a subsumption strategy allowing your robot to search across the environment while avoiding obstacles. 
	\item To verify that your robot is performing as you intend, design some unit tests that demonstrate appropriate switching between different behaviours.
	\item Consider what parameters to optimise to improve performance.
\end{todolist}

{\bfseries Question \myq\ Describe your subsumption architecture, tests that you have implemented, and performance improvements that led to them.   \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}

Our robots subsumption architecture currently works in the following manner:\par
- PID obstacle avoidance is at the bottom and subsumed by search behavior, however, it will subsume the search behavior if the distance measurement comes to within 5cm. \par
- The search behavior will subsume the PID avoidance behavior when the robot is not actively avoiding anything. It will direct the robot to areas of higher light intensity. We have also prepared for it to be subsumed by the search behavior once the light value rises above a certain threshold (a value of 35 is being used temporarily).

Unit tests:\par
We completed some unit tests to ensure the subsumption architecture is working as intended.

\begin{tabular}{ |p{2cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : Subsumption architecture} \\
 \hline
 Test & Reasoning & Expected results & Actual results & notes \\
 \hline
 Placing the robot before an obstacle & To check whether the PID avoidance will be used to avoid obstacle & Robot will avoid obstacle & Worked as intended & \\
 \hline
 Placing the robot in the middle of the maze & To check whether the robot will use the most appropriate behavior for the current state & A behavior suitable to the current state will be activated & Worked as intended & We put a print statement on each behavior so we knew when the correct behavior was being activated. PID avoidance was only used when close to obstacles as expected.\\
 \hline
 Putting the robot in front of light source & To check the correct behavior takes over, without the robot trying to avoid the light source & Robot stops and does not avoid light source & Robot did as intended & We temporarily made a simple stop (speed set to 0 on both motors) behavior, so we knew the correct behavior would be used \\
 \hline
\end{tabular}


The unit tests above show that the subsumption architecture implemented in the robot is working as necessary. 

An issue we identified during these tests is the light value threshold, where the robot commonly mistook the value from the window as the light source, activating the temporary stop behavior. Hence we did some unit tests to determine the right values to use in our program to activate different behaviors. However, during testing, we realized it was more efficient to build a container for the light sensor which only let in light from the front. When we began testing this design (see table below) our values were lower than anticipated, and we realized it was because light is emitted from the floor whist our sensor was higher up and we were blocking light from the bottom with the container. We then changed the design to allow light from the bottom as well. Results of this change can be seen in the table below.

\begin{tabular}{ |p{2cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : color sensor} \\
 \hline
 Physical configuration & Light source & Window & Arena corner & Notes\\
 \hline
 Uncovered & Mean = 38.8, std = 3.2 & Mean = 34.6, std = 4 & Mean = 13, std = 3.1 & \\
 \hline
 Covered & Mean = 17.6, std = 1.9  & Mean = 8.8, std = 1.7 & Mean = 3, std = 1.9 & \\
 \hline
 Bottom plaque gone & Mean =24.6, std = 3.2 & Mean = 11, std = 1.9 & Mean = 2, std =1.3 & \\
 \hline
\end{tabular}

From the results above, it is clear that by covering the light sensor we have been able to get more trustworthy values and reduced the likely hood of the robot mistaking a window as the light source.

Carrying on we will be using a value threshold of 17 to start our beaconing behavior, and a value of 24 to activate our stopping behavior.

\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding A Stop Behaviour}

In the final competition, the individual's location can be identified by an alarm beacon (omnidirectional light source) that can be used to know when your robot has found the target.

\begin{todolist}
	\item In your robot kits you will find a Colour Sensor which can be used to classify colours, measure reflected light intensity or measure the ambient light intensity.  You should mount this sensor on your robot and connect to the EV3 using the same port array where your ultrasonic sensors are already connected.
	\item Refer again to the EV3 documentation to find the command that allows you to access the Colour Sensor.  You should use Ambient Light Intensity mode.  
	\item Document light readings in various parts of the arena - far away from the light, close to the light (remember that conditions might adapt due to the windows - can you document this?). 
	\item Using the above information consider where to position the Colour Sensor on your robot and any adaptations that you might make to your robot morphology to improve the signal. 
	\item You should now have a sufficiently good understanding of the information available to program a stopping behaviour for your robot. Program this and add it as another layer of your subsumption architecture.     
\end{todolist}	
	
{\bfseries Question \myq\ Describe the stopping behaviour including describing your hardware and software solutions, any calibration attempted and any unit testing performed.  Also describe if you added via a subsumption architecture or some other control strategy \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Software design:\par
We initially stopped our robot when the light source was above a value of 24 (see color sensor unit tests in Question 3) and there was a suitable distance from an obstacle, however, due to the reflectivity of the arena walls, we had a few fake positives when the light source was close to the walls, or it was a particularly sunny day. This method also had some design flaws, such as the robot ignoring the light source when it passed right beside it (because it did not have a suitable distance value to activate the stop behavior). To overcome this issue, we created a "near light" function so when the light value is around 20 (indicating it is very close to the light source), the robot stops and does a 360 degree turn, then heads in the direction with the highest light intensity until the measured distance is less than 25cm. As can be seen from the unit testing below, this resulted in no more fake positives.

Unit testing:\par
We did 6 tests for each stopping behavior version made, keeping the tests the same for each version so it was fair. 

\begin{tabular}{ |p{4cm}|p{2.5cm}|p{2.5cm}|p{2cm}|  }
 \hline
 \multicolumn{4}{|c|}{Unit tests : stopping behavior} \\
 \hline
 Method & True positives  & False positives & Notes \\
 \hline
 Stopping with light value and distance measurement & 5 & 1 & \\
 \hline
 Stopping behavior aggregated with "near light" function & 6 & 0 & \\
 \hline
\end{tabular}

As can be seen from the tests above, the final version we made effectively eliminated false positives for the time being, though it did end up adding some extra time (~8 seconds) before the robot fully stopped.

Additional insights:\par
An issue we found during our unit tests of the software design, is due to the way we've coded the algorithm without a gyro sensor, when the robot is rotating, it will only stop when it reaches the same value it started with, which means if it gets dislocated during the spin to be further away from the light, it will not stop until it closer to the light. We would have liked to make this function more robust by adding the gyro sensor, but due to time constraints, because it works we've left it as is.
We would have improved our hardware design by lowering the color sensor which is currently placed too high in the robot (which also introduces errors to the ultrasonic sensor). Currently this is unavoidable because it was the only way of inserting the rotation motor at the front of the robot. One way we've mitigated this is by leaning the rotating sensor down but this introduces some discrepancies while taking value measurements. Hence, we would have liked to find a way to inverse the rotation motor so the color and ultrasonic sensor would be at the bottom of the robot, but the hardware design of this would be too complex to implement with the time we have.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding a Beaconing Behaviour}

Your robot now has all the components required to solve the task: search; obstacle avoidance; and stop criteria.  However, performance can be improved further by appreciated that the light also provides information that your robot can use to approach the light source. You are now going to add this beaconing behaviour to your robot.

\begin{todolist}	
	\item Consider how the light provides information that could be used to guide your robot to the target.  Also consider how you would design a control algorithm that will drive your robot towards the light source (e.g. up a gradient) using a single sensor. You will need to work as a group to develop both a robot morphology and control algorithm to perform this task.  \textit{You might want to refer to material covered in Lecture 6.} 
\end{todolist}

{\bfseries Question \myq\ Describe the design of your light following behaviour including describing your hardware and software solutions, any calibration attempted and any unit testing performed.   \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Software design:\par
When our wanderer search records a light value above 17, it assumes it is roughly facing the direction of the light source. It proceeds by doing a 360 degree turn while recording light values (same as the wanderer search function appended at the end), and heads in the direction of highest light intensity in a wavy motion (like a snake); this ensures that it is always on a path with highest light value, and if it isn't, it adjusts itself. This should result in the robot being on a direct path to the light source. We did this as during the stop behavior, we found the robot would commonly go past the light.
Once it is close to the light so that the light value passes a threshold of 24, the stopping behavior subsumes the beaconing behavior.

Unit tests:\par
To ensure the beaconing behavior is working as intended, we carried out some tests:

\begin{tabular}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{4}{|c|}{Unit tests : Beaconing behavior} \\
 \hline
 Test & Reasoning & Expected results & Actual results \\
 \hline
 Putting robot close (300cm) to light source & To ensure stop behavior takes over & Robot executes stop behavior & Robot executed stop behavior \\
 \hline
 Putting robot 1 meter from light source & To test beaconing behavior finds the light & Robot makes it's way to source, and stop behavior then takes over & Robot did as intended \\
 \hline
 Putting robot left of light source at a distance of a meter & To test whether robot can correct it's trajectory & As robot makes its way to the source, it adjusts it's path & Path was adjusted \\
 \hline
 Putting robot close to an obstacle next to light source & To test whether PID avoidance takes over when needed & Robot avoids obstacle and then beaconing behavior takes over & Robot did as expected \\
 \hline
 Putting robot just left of the light source at a distance of 400cm & To test whether it can find the light source & Robot does 360 scan due to high light level, and turns towards light, where stopping behavior takes over (due to proximity) & Robot did as expected \\
 \hline
\end{tabular}

We're really happy with how well the beaconing behavior seems to work as can be seen from the unit tests.

Additional insights:\par
We have done the most we can to decrease the likely hood of the robot passing next to the light without noticing, which is why we made the threshold to do a 360 so low, but we haven't implemented any fail safes to prevent this from happening. If we had more time, we would have liked to implement a feature where the robot keeps an average of the last few measurements (maybe 100), and if it notices a sudden drop in the light value (indicating it just passed by the light source_, it triggers a 360 scan. Whilst this seems easy to implement, we did not have the time to implement this and test that it worked effectively.


\end{mdframed}
\vspace*{\baselineskip}

\subsection{Final Multi-layer Control Architecture}

You will need to incorporate this new beaconing behaviour into your robot control strategy.   

\begin{todolist}	
	\item Can this behaviour be easily added to the subsumption architecture used previously?  Or is an alternative method now required? (\textit{you might want to refer to material covered in Lecture 4.}) In your team you should develop a multi-layer control strategy allowing your robot to complete the final task for the Search and Assist competition.  Use unit testing to assess if your design choices were correct and justify any changes made.
\end{todolist}

{\bfseries Question \myq\ Describe your final multi-layer control architecture. You should discuss if you chose to stick with a subsumption architecture or use an alternative and justify this design choice.  Support your conclusions with data from unit tests. \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}

Our final subsumption architecture has the following hierarchy:\par
- At the top is the stop behavior, which subsumes any previous behavior as long as the light value recorded surpass 24. This behavior does a last scan to ensure it is heading in the right direction, and then stops when the measured distance reaches 25cm.\par
- Below the stop behavior is the beaconing behavior, which subsumes the wanderer search when the light value surpasses a value of 17. When it takes over, it does a 360 scan, and in a wavy motion, heads in the direction of the highest light value until it is subsumed by the stop behavior (if for some reason it isn't, the PID avoidance takes over).\par
- Below the beaconing behavior, is the wanderer search which is activated when a light value above 17 is recorded. The wanderer search randomly heads in the direction of high light intensity. Every 30 execution cycles, it does a 360 scan to ensure it is heading in a direction of high light intensity.\par
- At the bottom is the PID avoidance, which subsumes any behavior when the distance measurement is at 5cm or less.

We've chosen to stick to a subsumption architecture because we unknowingly coded our robots behaviors to follow suit perfectly. The robots behavior has been decomposed into suitable sub-behaviors that are sequential and build off each other. The robot works in a very bottom up fashion, increasing the complexity of behaviors when it is deemed necessary. It also works by slowly subsuming lower behaviors, as was mentioned in the beaconing behavior.

Below we've done some unit tests to ensure the robot is prepared for the final competition:

\begin{tabular}{ |p{2cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : subsumption architecture tests} \\
 \hline
 Test & Reasoning & Expected results & Actual results & notes \\
 \hline
  Putting robot before an obstacle next to the light source & To test the interaction between the different behaviors & The robot should use PID avoidance, then wanderer search to find the light source, then beaconing and stopping behavior when at the light & Robot did as intended and stopped at the light & \\
 \hline
 Placing the robot 5cm from light & To check that the stop behavior takes over & All behaviors except stop behavior are ignored & Beaconing behavior was activated where the robot did a 360, then stop behavior took over & Even though the expected outcome did not come to pass, the robot still did not avoid the light, so we did not make any changes. With more time we could have improved this. \\
 \hline
 Placing the robot 5cm in front of an obstacle & To ensure obstacle avoidance activated and correct behaviors are then used & Obstacle avoidance is used, then search behavior takes over & Did as intended.\\
 \hline
 Placing the robot in the middle of the arena & To ensure our robots behaviors work together to find the light source & Robot successfully finds light source & Expected outcome came to pass & There were a few collisions with moving obstacles, but when there was no interference, the robot avoided stationary obstacles quite well \\
 \hline
 Putting the robot in the opposite side of the arena, facing away from the light & To test how robot does with very unfortunate start & Robot eventually finds the light, with no collisions or rescues & Expected outcome came to pass & \\
 \hline
\end{tabular}

From the unit tests above, whilst the robot appears to be ready for the final competition, there are certain areas we would have liked to improve given more time, such as in the second test, where the search behavior took over instead of stopping behavior.
However, the tests above prove that the robot is ready for the competition, so we will not make anymore changes.

Our unit tests also demonstrate that different behavior subsume each other as intended, proving the subsumption architecture is working correctly.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Summary}

Your robot is now ready to compete in the final competition!

\newpage
\section{Part II: Search and Assist Competition}

\subsection{The Competition}

As for Assignment 1, the competition will be organised into a number of leagues, each consisting of several teams (and their robots).  Marks will be awarded based on each robot's performance \textbf{within its league}.  This will mean that each team will be competing against teams that have had an equal amount of time devoted to the development of their respective robots.  The competition will take place during the final lab session.

{\bfseries Note:}  \emph{If there is time at the end of the league battles, the winning robot in each league will compete again to find the overall champion.  This `championship' contest will not count towards the marks for the assignment, but a (small) prize may be awarded.}

As you can imagine, running such an event with a large number of teams/robots requires very precise time management, so we will be issuing a strict timetable for the final lab session.  This should appear on MOLE the week before.  It is essential that you prepare carefully for your designated time slot (otherwise you may lose marks - see below).

The rules for the competition are as follows:
\begin{enumerate}
	\item Each team must register their arrival at the arena (with their robot) 10 mins prior to their league's designated time slot, after which no further technical development will be permitted.
	\item Each team will be called forward in turn to place their robot on the starting position.
	\item An `official' run for each team's robot will be timed by the lab demonstrators / lecturers.  They will record the accumulated time that the robot spends in the designated target area surrounding the light source during the 2 minutes allowed.
	\item Each `touch' with a wall or obstacle will incur a \textbf{-10 second penalty}.
	\item If a robot needs to be `rescued', a \emph{designated} team member may place it back in the arena the course at the location where things went wrong.  However, the clock will keep running.
	\item The designated rescuer must stand \emph{outside} the arena next to the starting position, and return to that position after each rescue.
	\item Up to two rescues are permitted.
	\item Each rescue will incur a \textbf{-20 sec penalty}.
	\item A third rescue is NOT permitted.  Instead the run will be terminated and the total time accumalated at the target recorded.
	\item Teams will be ranked in their league according to their accumulated time at the target (minus any penalties).  [what to do with people clustered on 0min?]
\end{enumerate}
	
Marks will be awarded as follows:
\begin{itemize}
	\item 15 marks will be awarded to the team with the best run within their league, 14 marks to the second best team, and so on.  
	\item 5 \emph{bonus} marks will be awarded for a robot that stops within the target location within the 2 minute time limit with \emph{no} wall or obstacles touches and \emph{no} rescues.
	\item A team who fails to appear at the starting position will receive 0 marks.
\end{itemize}


\subsection{Your Team's Performance}

{\bfseries Question \myq:}  \emph{What was the result of your official attempt?} (Worth up to 20 marks)\\
\begin{mdframed}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } \hline
		 \bf{Total Time at Target} & \bf{Wall/Obstacle Touches} & \bf{Rescues} \\ \hline
		 01:09 & 0 & 0 \\ \hline
	\end{tabular}
\end{center}
\end{mdframed}
\vspace*{\baselineskip}

{\bfseries Note:}  \emph{The demonstrators will have already recorded the above information.  However, please include it here as a cross-check.}

Finally, how did you organise your team?  I.e.\ what was each member's role and responsibility, and what was each person's contribution as a \% (adding up to 100\%)?  Please fill in the Table below:
\begin{center}
	\begin{tabular}{ | c | p{8cm} | c | } \hline
		 \bf{Team Member} & \bf{Role} & \% \\ \hline
		Hubert baran, 170164182 & Robot coder & 33.3\ldots \\ \hline
		Jose Alves, 170163532 & Report & 33.3\ldots \\ \hline
		Jinhan Shi, 170205245 & Robot coder & 33.3\ldots \\ \hline
	\end{tabular}
\end{center}



\end{document}

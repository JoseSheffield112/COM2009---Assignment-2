%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% COM2009-3009 Human-Machine Interaction and Robotics
% Lab Assignment #2
% Prof. Roger K. Moore & Dr. Mike Mangan
% University of Sheffield
% 5 March 2019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[hidelinks,a4paper,11pt]{article}

\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{mdframed}
\usepackage{enumitem,amssymb}
\usepackage{float}
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}
\hspace{-2.5pt}}
\newcounter{question}
\newcommand\myq{\refstepcounter{question}\thequestion}
\usepackage{gensymb}


\begin{document}


\begin{titlepage}

\begin{center}
{\LARGE University of Sheffield}\\[1cm]
\huge {\bfseries COM2009-3009\\Human-Machine Interaction and Robotics}\\[1cm]
\includegraphics[width=5cm]{tuoslogo.png}\\[1cm]
{\LARGE Lab Assignment \#2\\\textbf{`Multi-Layer Control'}}\\[2cm]

% DELETE THESE LINES FOR THE STUDENT VERSION
%vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
%{\huge \bfseries Instructions}\\[0.5cm]
%{\Large Prof. Roger K. Moore \& Dr. Mike Mangan}\\[1cm]
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

% UNCOMMENT THESE LINES FOR THE STUDENT VERSION
%vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv
%% EDIT YOUR NAMES
%{\Large John Smith}\\
%{\Large David Jones}\\
%{\Large Scott Macdonald}\\[1cm]

%% EDIT THE ID OF YOUR ROBOT
%{\LARGE Robot: XY}\\[1cm]
%^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

{\LARGE Department of Computer Science}\\
{\Large \today}
\end{center}

\end{titlepage}

\tableofcontents
\newpage

\section{Overview (please read carefully)}

In Lab Assignment \#1 you investigated a single-layer negative-feedback control system.  In particular, you developed a `Proportional-Integral-Derivative' (PID) controller that was capable of steering a robot through a maze consisting of a long winding corridor.  For Lab Assignment \#2 your challenge is to navigate a more complex environment containing real-world hazards - and, in order to do this, you will need to add new layers of functionality to your existing robot through a  \emph{multi-layer} control system.

This programming assignment is worth 30\% of the overall course mark.

\subsection{Logistics}

As before, the lab classes for COM2009-3009 take place in Computer Room 3 in the Diamond (DIA-CR3) from 13:00 to 14:50 on Mondays (for Group A) and 09:00 to 10:50 on Wednesdays (for Group B).  \textbf{The teams have been re-randomised from Assignment 1.  Your new team and robot are listed on MOLE}.

The assignment will again be judged in a league-based competition, and assessed on the basis of a group report (prepared using \LaTeX).

The deadline for handing-in the group report (via MOLE) is \ldots

\begin{center}\textbf{{\large {\color{red}Midnight Friday 17\textsuperscript{th} May 2019 (week 12)}}}\end{center}

{\bfseries Reminder:}  \emph{You may \underline{not} take a robot out of DIA-CR3 under any circumstances!}

{\bfseries Reminder:}  \emph{Please be careful in and around the robot arena; the robots can move unexpectedly and the arena itself is high enough to be a trip hazard.  There is a risk assessment for this in the folder on the teaching podium.}

{\bfseries Reminder:}  \emph{At the end of each lab session, turn off your robot and return it to the correct charging cabinet, with the microcomputer (brick) being placed on charge by plugging in the power adapter in the centre section of the cabinet.}

{\bfseries Reminder:}  \emph{If you have customised your robot, take a photo (as a memory aid) and return it to its original `Driving Base' configuration and place all the Lego back in its box\footnote{This is necessary because, as before, the robots are shared between Group A and Group B.}}.


\subsection{Provided Materials}

In addition to these instructions, you have been provided with a \texttt{.zip} file containing a number of items\footnote{The provided \texttt{SystemDiagram.jpg} is simply a placeholder that you will replace with your own diagram.} that you will need for this assignment.  In particular, you have been supplied with a \LaTeX\ template - \texttt{COM2009-3009\_Assignment-2\_Robot-XY.tex} - which will you use to compile your response sheet.

\begin{todolist}
	\item Before you start working with \LaTeX, edit the filename of the provided \texttt{.tex} file by replacing \texttt{XY} with your robot id.
\end{todolist}

As before, your edited \texttt{.tex} file should compile to produce a \texttt{.pdf} document with your names on the front cover, followed by your responses to a number of questions.  You will be submitting the \texttt{.pdf} file when you have completed the assignment, \emph{not} the \texttt{.tex} file.

\subsection{Hand-In Procedure}

Once you have completed the assignment, one member of each group should submit your response sheet (in \texttt{.pdf} format) via MOLE.  You do NOT need to submit a paper copy, and do NOT submit your \LaTeX\ source files.

The filename should be of the form \texttt{COM2009-3009\_Assignment-2\_Robot-XY.pdf}.  Please make sure that your names and the id of your robot are shown correctly on the front page of your report.

Standard departmental penalties apply for late hand-in\footnote{\url{http://www.dcs.shef.ac.uk/intranet/teaching/public/assessment/latehandin.html}} and plagiarism\footnote{\url{http://www.dcs.shef.ac.uk/intranet/teaching/public/assessment/plagiarism.html}}.

Feedback (including provisional marks) will be provided via MOLE within three working weeks of the hand-in date.

The deadline for handing-in this assignment (via MOLE) is \ldots

\begin{center}\textbf{{\large {\color{red}Midnight Friday 17\textsuperscript{th} May 2019 (week 12)}}}\end{center}

\subsection{Increase in expected levels of autonomy}
In Lab Assignment \#1, care was taken to guide you through the various aspects of design that were required to build a maze-running robot.  This included an introduction to the Lego EV3 hardware, sample code for programming your robot, and detailed guides on how to calibrate and program a PID controller.  

In Lab Assignment \#2, we expect teams to work in a more independent manner to find their own solutions to the problems posed.  Part I below provides guides as to the type of behaviours you will need to implement and also links to lectures where common solutions to similar problems were covered.  We strongly encourage students to research novel solutions, or even come up with their own.  Novel solutions can be discussed in the submitted report.

Also, we cannot guarantee extra lab sessions as for Assignment 1 so please plan accordingly.  It is very important to manage your development time on each task to ensure that you get to the end in time for the competition.

{\bfseries Reminder:}  \emph{Lab Demonstrators are provided to guide your development and address technical questions that you might experience.  Given the expectation of independent development they will not be able to debug code or provide full solutions as they will be specific to your robot.}



\newpage
\section{Part I: Building A Multilayer Robot Control Architecture}

\subsection{The Search and Assist Challenge}

In Assignment 2 you will again develop a robot to compete against your peers in a simulated life-like challenge within the same test arena as Assignment 1.  Your task is to program a 'search and assist robot' that must find a stricken individual in an unstructured environment (test arena with randomly distributed obstacles) and remain there until further assistance arrives.  The individual's location can be identified by an alarm beacon (light source). Your robot must navigate to the individual's location while avoiding contact with obstacles and the arena walls.  Your robot's start position, and the positions of the target and obstacles will not be revealed until the competition session so you will have to design a controller that works in many conditions.  You shall be ranked by the number of seconds that your robot spends at the target location within the 2 minutes allowed for each robot.

The following sections guide you through development of the various control behaviours that your robot will require to resolve this challenge but we expect and encourage teams to develop novel solutions that improve performance further.  You will also notice that there is increased requirement to design and document unit tests to demonstrate the functioning of your subsystems.  The design of the unit tests is for you to define - as you would in a real engineering or R\&D position. 

\subsection{Building An Obstacle Avoidance Behaviour}

In Assignment 1 you developed a PID controller to follow a maze without touching walls i.e. avoiding obstacles.  In your new group, use the lessons learned from the last assignment to give your robot an obstacle avoidance capability.  

\begin{todolist}
	\item Program a PID-based obstacle avoidance behaviour.  Take into account the hardware designs that produced the best results in the maze following competition.  Refer to Assignment 1 documentation for reminders on calibrating your controller.
	\item Design and document unit tests to demonstrate that your robot succeeds in this task.  
\end{todolist}

{\bfseries Question \myq\ Describe your PID-based obstacle avoidance architecture with accompanying unit tests that demonstrate that it functions correctly. \emph{(worth up to 5 marks)}}
\\
\begin{mdframed}
Design:\par
Hardware:\par
We initially used the predominant lab 1 robot configuration (front facing perpendicular ultrasonic sensors), as we were all familiar with this configuration and had the same working PID code. However, during the unit testing (see below), we had a few instances where the robot hit an obstacle head on due to it being in it's blind spot. We decided to make a new design so we could aim for the extra 5 marks of avoiding obstacles.
The new design uses a rotating ultrasonic sensor, that continuously alternates between scanning the front, right and left of the robot. Whilst this method still has a distinctive weakness of reacting slower to obstacles on a side the robot isn't facing, we have taken steps to decrease the occurrence by limiting the speed and increasing the minimum distance in the PID software implementation (tests below).
In the end we went with the second implementation, as it allowed us to overcome 2 weaknesses of the first design. Firstly, we significantly reduced the robot blind spots; furthermore, if our robot has a wall in front of it, this method scans both sides without moving to assess the best side to turn to, instead of colliding with the wall.

Software design:\par
For the first hardware design we used our previous labs PID avoidance code, which subtracted the left measurement from the right and used this value to calculate the wheel speed (this method introduced some errors in large spaces, as this PID method is most suitable to tight spaces, which we have overcome in our second implementation).
We then adapted the PID code from our first lab assignment so that the robot wheels move at a base speed; if a value from one of the robots sides is within the minimum distance, it will calculate a value with that distance and deduct it from the opposite wheels base speed (similar to the Braitenberg "explorer robot"). When the front value is within the minimum distance, the robot stops and backs ups, then measures its left and right side; it then moves to the side with the highest value, favoring the left if both values are at the sensor maximum.

Unit tests:
We performed the same tests for each method, so by the end we could make a clear decision on the most preferable avoidance system.

\begin{tabular}{ |p{2cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : Ultrasonic sensors perpendicular} \\
 \hline
 Scenario & Reason & Expected outcome & Actual outcome & Notes\\
 \hline
 Placing the robot in the middle of the arena & To test if PID avoidance code is working & Robot moves and avoids any oncoming obstacles & Robot moved and avoided obstacles & We ran this twice, as in first test it collided with an obstacle in its blindspot as it was moving too fast.\\
 \hline
 Placing the robot facing a corner & To test whether the robot can get itself out without a collision & The robot gets itself out of the corner with no collisions & The robot collided with the wall a few times and needed a rescue & N/A. \\
 \hline
 Placing the robot in front of an obstacle& So we can get a feeling of how large the blind spot is & Collides with the obstacle & The robot collided with obstacle & We did a few of these tests, varying the distance to the obstacle (so it could move into an angle that could detect the obstacle) and weren't satisfied with the results. \\
 \hline
\end{tabular}

Due to the faults found with the previous implementation of the robot which were highlighted previously, we pursued a different design.
We then decided to  test the robot with the same tests of the previous design.

\begin{tabular}{ |p{2cm}||p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : Rotating ultrasonic sensor} \\
 \hline
 Scenario & Reason & Expected outcome & Actual outcome & Notes\\
 \hline
 Placing robot in the middle of the arena & To test whether PID avoidance code is working & Robot moves about arena avoiding obstacles & Robot moved and avoided obstacles & There were a few instances were other robots crashed into it on its blind spot, but it reacted well to stationary obstacles \\
 \hline
 Placing the robot in a corner & To see if programmed behavior works & Robot should back up and turn another direction & Robot did as intended & N/A \\
 \hline
 Placing the robot in front of an obstacle & To test one of the main flaws in previous design & Robot backs up and turns another direction & Robot did as intended & \\
 \hline
\end{tabular}

 It is clear from the 2 unit tests that our second design is working best. Whilst there are some weaknesses to it, such as the constant shaking of the sensor introducing further uncertainties to an already spotty sensor, and the slow reaction times, all designs have inherent flaws that cannot be overcome with the time and resources we have, so we're happy enough with the current design.
 
 
 Additional insights:\par
 If time allowed, we would have explored a way of increasing the accuracy of the measurements (potentially by the 2 rotating sensors opposite each other). However, the extra time needed to implement this with the PID software would take too long.
 Furthermore, another solution we would have liked to have tested (idea collected from dyson hoover lecture) would be to use a touch sensor on both sides of the robot, so when it's close to a collision on the side, the sensor would introduce this input and the robot would steer away from it. This solution would easily overcome burden placed on the ultrasonic sensor as well as reducing the probability of a collision.
 
 However, to further the implemented design without hardware changes we'd have liked to implement 2 new features that we did not have time to develop. Firstly, we would have liked to increase the snapshots of distance the sensor captures - currently, we an only rotate between the 3 sides roughly every 4 seconds due to the weakness of the motor, and if increased further, it becomes very unstable and we're not even sure we could this with PID coding. A second feature we'd like to add is making an internal map, where the robot keeps a "picture" of each distance from an obstacle in the 180 degrees in front of it, and considers the distance in the direction its currently moved.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding A Search Behaviour}

Your robot will now be able to react to obstacles but needs a guidance strategy to explore the arena in search of the individual (light).  Thus, your task is to add a search behaviour allowing your robot to explore the environment.

\begin{todolist}
	\item Program a search behaviour that will drive your robot around the arena.  Consider the various search algorithms that you might use to most efficiently explore the test arena.  \textit{Hint: search strategies were covered in Lecture 6 but there are many others available.}
	\item In your robot kits you will find a Gyro Sensor which can be used to monitor turning angles and angular accelerations.  This may or may not be useful to your specific search strategy.  Refer to the online EV3 documentation (\url{https://ev3dev-lang.readthedocs.io/projects/python-ev3dev/en/stable/spec.html}), to understand how to access the data from the Gyro Sensor.
	\item Again, design and document unit tests to demonstrate that your search strategy performs as expected.  If time allows, you may want to implement and compare two different search algorithms to assess which is best suited to the task, or even implement a novel hybrid search strategy. 
\end{todolist}

{\bfseries Question \myq\ Describe the design of your search algorithm including discussion of design choices and what testing you have implemented. \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Hardware design:\par
For our spiral search we secured a light sensor at the front of the robot to minimize the swaying of the robot when doing sharp turns so our values were more accurate. We placed the sensor at the front of the robot so there was no bias (as placing it on a side will blind to values from the opposite side).
When using the wanderer search, we paired our light sensor with the ultrasonic sensor on the rotation motor, so it's easier for the robot to take recordings of light values around it.

Algorithm design:\par
We initially coded a simple spiral search with the spiralling radius gradually increasing over time, where the beaconing behavior would take over once the robot is close enough to light source.
We felt confident the robot could eventually find the light source with this search pattern, however due to the time restriction and the varying map layout (which could prevent us from effectively using a spiral search), we decided to code another pattern so we could compare the two and be confident the one chosen was best.
We decided to compare the spiral search to a wanderer search. This search pattern uses the PID avoidance system aggregated with a color sensor, to "wander" around the arena whilst favouring areas with higher light intensity. This search pattern is best suited to our robot due to the rotating ultrasonic sensor.

Spiral search:
3:46, 2:57, 35,  1:05
wanderer search:
1:49, 1:22, 9, 29
Below are the tests comparing both algorithm design we made. We did 4 trials for each pattern, 2 close to the source and 2 far from the source, and measured how long it took for the robot to reach a light value above 35, where the beaconing behavior would then take over.\par
Unit tests:


\begin{tabular}{ |p{3cm}|p{3cm}|p{2.5cm}|  }
 \hline
 \multicolumn{3}{|c|}{Unit tests : spiral search vs wander search} \\
 \hline
 Method & Time (n=4) & Notes\\
 \hline
 Spiral search & Mean = 2:05 & \\
 \hline
 Wanderer search & Mean = 57 & \\
 \hline
\end{tabular}

It is worthwhile to consider that all these tests were conducted during lab times, when everyone else was also using their robots, hence, there is a certain amount of interference in the times. However, it is abundantly clear that the best search pattern we can use is the wanderer search.

During later stages of development we aggregated a function which every 30 execution cycles, will do a 360 degree spin on the spot, recording a light value about every 5 degrees, and then heads in the direction of most light. We did this for two reasons; firstly, to ensure that our robot is heading the correct direction and won't miss the light by passing by it's side. Secondly, so that if the robot is getting stuck around the same area, this should help it to head a new direction.

Additional insights:\par
Like the PID avoidance issue, the robot would work much more efficiently if it was able to collect more light samples, which would require the rotation motor to move faster.
Furthermore, one of the design issues we have is we placed the light sensor at the top of the robot due to the rotating motor, however, the light source emits light from the bottom half at the floor. Hence, the robot would collect much more accurate light values if the sensor was at the bottom half of the robot.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Switching Behaviours Using A Subsumption Architecture}

Your robot should now have two functioning, single layer behaviours:  obstacle avoidance and search.  You now need to consider how your robot will select which strategy to apply and when.  

\begin{todolist}
	\item With reference to lecture 4 implement a subsumption strategy allowing your robot to search across the environment while avoiding obstacles. 
	\item To verify that your robot is performing as you intend, design some unit tests that demonstrate appropriate switching between different behaviours.
	\item Consider what parameters to optimise to improve performance.
\end{todolist}

{\bfseries Question \myq\ Describe your subsumption architecture, tests that you have implemented, and performance improvements that led to them.   \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.  Replace this text with your answer.
\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding A Stop Behaviour}

In the final competition, the individual's location can be identified by an alarm beacon (omnidirectional light source) that can be used to know when your robot has found the target.

\begin{todolist}
	\item In your robot kits you will find a Colour Sensor which can be used to classify colours, measure reflected light intensity or measure the ambient light intensity.  You should mount this sensor on your robot and connect to the EV3 using the same port array where your ultrasonic sensors are already connected.
	\item Refer again to the EV3 documentation to find the command that allows you to access the Colour Sensor.  You should use Ambient Light Intensity mode.  
	\item Document light readings in various parts of the arena - far away from the light, close to the light (remember that conditions might adapt due to the windows - can you document this?). 
	\item Using the above information consider where to position the Colour Sensor on your robot and any adaptations that you might make to your robot morphology to improve the signal. 
	\item You should now have a sufficiently good understanding of the information available to program a stopping behaviour for your robot. Program this and add it as another layer of your subsumption architecture.     
\end{todolist}	
	
{\bfseries Question \myq\ Describe the stopping behaviour including describing your hardware and software solutions, any calibration attempted and any unit testing performed.  Also describe if you added via a subsumption architecture or some other control strategy \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Hardware design:\par
For our search algorithm we paired the light sensor with the ultrasonic sensor.
Whilst developing our stop behavior, we built a container for the light sensor, so it only allowed in light through the front.
We did some tests (see below) and realized it was most efficient removing the bottom cover, but leaving the other 3.

Software design:\par
Initially 

uncovered:\par
38, 43, 35, 42, 36
covered:\par
5,5, 7, 6, 8
1.44+1.44+0.64+0.04+3.24
bottom half gone:\par
14, 18, 15, 17, 13
46.24+0.64+14.44+3.24+33.64

The unit tests below compares the color sensor values (4 samples each) facing the window and facing the source.\par
\begin{tabular}{ |p{2cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|  }
 \hline
 \multicolumn{5}{|c|}{Unit tests : color sensor} \\
 \hline
 Method & Light source & Window & Notes\\
 \hline
 Uncovered light sensor & Mean = 38.8, std = 3.6 & Mean = , std = & \\
 \hline
 Covered light sensor & Mean = 6.2, std = 2.6  & Mean = , std = & \\
 \hline
 Bottom plaque gone & Mean = , std = & Mean = , std = & \\
 \hline
\end{tabular}
As we can see above, adding a container lower the standard deviation and also decreased the mean values, which helps to better differentiate the sample from the windows.



\begin{comment}
Our stopping behavior uses the light values (when not covered, we found 
a mean of 37, with a deviation of about +/-4, and when covered, we found
it to be 15 +/-3, which is much more preferable, and it had
to directly face the window to get  value above 10 - but this 
is with cloudy weather) and the distance
(of ~400 cm) to stop within the light

\end{comment}


\end{mdframed}
\vspace*{\baselineskip}

\subsection{Adding a Beaconing Behaviour}

Your robot now has all the components required to solve the task: search; obstacle avoidance; and stop criteria.  However, performance can be improved further by appreciated that the light also provides information that your robot can use to approach the light source. You are now going to add this beaconing behaviour to your robot.

\begin{todolist}	
	\item Consider how the light provides information that could be used to guide your robot to the target.  Also consider how you would design a control algorithm that will drive your robot towards the light source (e.g. up a gradient) using a single sensor. You will need to work as a group to develop both a robot morphology and control algorithm to perform this task.  \textit{You might want to refer to material covered in Lecture 6.} 
\end{todolist}

{\bfseries Question \myq\ Describe the design of your light following behaviour including describing your hardware and software solutions, any calibration attempted and any unit testing performed.   \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}
Design:\par
Software design:\par
We initially carried on the search behaviour during our beaconing phase, as we believed it was good enough to find the light. However, we noticed that our robot was sometimes a little too much to the right or left of the light, which resulted in it going past the light.
To counter this and improve the robots performance we implemented an algorithm, which when the light value was above 12 (see final row of "unit tests : color sensor"), the robot would spin on the spot a full 360 and record values every 5 degrees, so it has an array of 72 values to choose from. It then picks the highest and turned to that direction, where we let our search algorithm take over again to go towards the light. Below are the results.


\begin{comment}
our beaconing behavior has 2 counterparts - firstly,
we use it in aggregate with the search behavior to direct the search

Secondly, when we notice the light level above a certain threshold(about 12)
we created a different function that will rotate in a 360, 
recording all light values, and finds the highest and follows it in a straight line

\end{comment}

\end{mdframed}
\vspace*{\baselineskip}

\subsection{Final Multi-layer Control Architecture}

You will need to incorporate this new beaconing behaviour into your robot control strategy.   

\begin{todolist}	
	\item Can this behaviour be easily added to the subsumption architecture used previously?  Or is an alternative method now required? (\textit{you might want to refer to material covered in Lecture 4.}) In your team you should develop a multi-layer control strategy allowing your robot to complete the final task for the Search and Assist competition.  Use unit testing to assess if your design choices were correct and justify any changes made.
\end{todolist}

{\bfseries Question \myq\ Describe your final multi-layer control architecture. You should discuss if you chose to stick with a subsumption architecture or use an alternative and justify this design choice.  Support your conclusions with data from unit tests. \emph{(worth up to 15 marks)}}
\\
\begin{mdframed}



\end{mdframed}
\vspace*{\baselineskip}

\subsection{Summary}

Your robot is now ready to compete in the final competition!

\newpage
\section{Part II: Search and Assist Competition}

\subsection{The Competition}

As for Assignment 1, the competition will be organised into a number of leagues, each consisting of several teams (and their robots).  Marks will be awarded based on each robot's performance \textbf{within its league}.  This will mean that each team will be competing against teams that have had an equal amount of time devoted to the development of their respective robots.  The competition will take place during the final lab session.

{\bfseries Note:}  \emph{If there is time at the end of the league battles, the winning robot in each league will compete again to find the overall champion.  This `championship' contest will not count towards the marks for the assignment, but a (small) prize may be awarded.}

As you can imagine, running such an event with a large number of teams/robots requires very precise time management, so we will be issuing a strict timetable for the final lab session.  This should appear on MOLE the week before.  It is essential that you prepare carefully for your designated time slot (otherwise you may lose marks - see below).

The rules for the competition are as follows:
\begin{enumerate}
	\item Each team must register their arrival at the arena (with their robot) 10 mins prior to their league's designated time slot, after which no further technical development will be permitted.
	\item Each team will be called forward in turn to place their robot on the starting position.
	\item An `official' run for each team's robot will be timed by the lab demonstrators / lecturers.  They will record the accumulated time that the robot spends in the designated target area surrounding the light source during the 2 minutes allowed.
	\item Each `touch' with a wall or obstacle will incur a \textbf{-10 second penalty}.
	\item If a robot needs to be `rescued', a \emph{designated} team member may place it back in the arena the course at the location where things went wrong.  However, the clock will keep running.
	\item The designated rescuer must stand \emph{outside} the arena next to the starting position, and return to that position after each rescue.
	\item Up to two rescues are permitted.
	\item Each rescue will incur a \textbf{-20 sec penalty}.
	\item A third rescue is NOT permitted.  Instead the run will be terminated and the total time accumalated at the target recorded.
	\item Teams will be ranked in their league according to their accumulated time at the target (minus any penalties).  [what to do with people clustered on 0min?]
\end{enumerate}
	
Marks will be awarded as follows:
\begin{itemize}
	\item 15 marks will be awarded to the team with the best run within their league, 14 marks to the second best team, and so on.  
	\item 5 \emph{bonus} marks will be awarded for a robot that stops within the target location within the 2 minute time limit with \emph{no} wall or obstacles touches and \emph{no} rescues.
	\item A team who fails to appear at the starting position will receive 0 marks.
\end{itemize}


\subsection{Your Team's Performance}

{\bfseries Question \myq:}  \emph{What was the result of your official attempt?} (Worth up to 20 marks)\\
\begin{mdframed}
\begin{center}
	\begin{tabular}{ |c|c|c|c|c| } \hline
		 \bf{Total Time at Target} & \bf{Wall/Obstacle Touches} & \bf{Rescues} \\ \hline
		 01:09 & 0\ldots & 0\ldots \\ \hline
	\end{tabular}
\end{center}
\end{mdframed}
\vspace*{\baselineskip}

{\bfseries Note:}  \emph{The demonstrators will have already recorded the above information.  However, please include it here as a cross-check.}

Finally, how did you organise your team?  I.e.\ what was each member's role and responsibility, and what was each person's contribution as a \% (adding up to 100\%)?  Please fill in the Table below:
\begin{center}
	\begin{tabular}{ | c | p{8cm} | c | } \hline
		 \bf{Team Member} & \bf{Role} & \% \\ \hline
		? & ? & ? \\ \hline
		? & ? & ? \\ \hline
		? & ? & ? \\ \hline
	\end{tabular}
\end{center}



\end{document}
